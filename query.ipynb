{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20294b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface-hub           0.35.0\n",
      "langchain                 0.3.27\n",
      "langchain-community       0.3.29\n",
      "langchain-core            0.3.76\n",
      "langchain-huggingface     0.3.1\n",
      "langchain-openai          0.3.33\n",
      "langchain-pinecone        0.2.12\n",
      "langchain-text-splitters  0.3.11\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: ENVIRONMENT SANITY CHECK\n",
    "!pip list | findstr \"langchain langchain-core langchain-huggingface huggingface-hub pinecone-client\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37a8a53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports and environment loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: IMPORTS AND SETUP\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "print(\"✅ Imports and environment loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9a9cbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings model initialized successfully.\n",
      "Model details: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: INITIALIZE EMBEDDINGS MODEL\n",
    "try:\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    print(\"✅ Embeddings model initialized successfully.\")\n",
    "    print(f\"Model details: {embeddings.model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error initializing embeddings model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2511d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retriever test successful. Found 3 relevant documents.\n",
      "--- Sample Retrieved Content ---\n",
      "Doc 1 (Page 13.0): 2025 BUDGET SPEECH \n",
      "10  |  NATIONAL TREASURY 2025 \n",
      " \n",
      " \n",
      " \n",
      "This budget also retains the provisional allocations for early retirement, allocations for PRASA and the \n",
      "municipal trading entity reforms announced before, but at a slightly lower level than a...\n",
      "Doc 2 (Page 21.0): 2025 BUDGET SPEECH \n",
      "18  |  NATIONAL TREASURY 2025...\n",
      "Doc 3 (Page 5.0): 2025 BUDGET SPEECH \n",
      "2  |  NATIONAL TREASURY 2025 \n",
      " \n",
      " \n",
      " \n",
      "INTRODUCTION \n",
      "Madam Speaker, a national budget is not merely an accounting exercise measuring what we earn, what \n",
      "we spend and what we borrow as a nation.  \n",
      "It is a reflection of the difficult  ...\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: TEST THE RETRIEVER\n",
    "try:\n",
    "    question = \"What are the main priorities mentioned in the budget speech?\"\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "    print(f\"✅ Retriever test successful. Found {len(retrieved_docs)} relevant documents.\")\n",
    "    print(\"--- Sample Retrieved Content ---\")\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        print(f\"Doc {i+1} (Page {doc.metadata.get('page', 'N/A')}): {doc.page_content[:250]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error testing the retriever: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf783a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hugging Face Inference Client initialized.\n",
      "\n",
      "--- FINAL ANSWER ---\n",
      " The main priorities mentioned in the budget speech are bolstering the state capability needed to deliver quality, reliable and sustainable core services, which is part of pillar three of the economic growth strategy. Despite making additional allocations, there are other long-standing spending pressures that cannot be funded within the current envelope. There was also a commitment to maintaining the current Value Added Tax (VAT) rate of 15%.\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: USING THE CORRECT CONVERSATIONAL TASK\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "\n",
    "# 1. Define the context from our previous retriever test\n",
    "context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "question = \"What are the main priorities mentioned in the budget speech?\"\n",
    "\n",
    "# 2. Create the prompt that will be sent as a user message\n",
    "# This is the same as before, just formatted as a single string\n",
    "prompt = f\"\"\"\n",
    "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "# 3. Initialize the official Hugging Face Inference Client\n",
    "try:\n",
    "    client = InferenceClient(token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"))\n",
    "    print(\"✅ Hugging Face Inference Client initialized.\")\n",
    "    \n",
    "    # 4. Call the model using the CORRECT method: chat_completion\n",
    "    response = client.chat_completion(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}], # <-- Format the input as a chat message\n",
    "        model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        max_tokens=512, # Note: parameter is max_tokens for this method\n",
    "    )\n",
    "\n",
    "    # 5. Extract the answer from the response object\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    print(\"\\n--- FINAL ANSWER ---\")\n",
    "    print(answer)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error calling the Hugging Face API directly: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
